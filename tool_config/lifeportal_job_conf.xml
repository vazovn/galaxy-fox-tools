 <job_conf>

     <plugins workers="4">
        <plugin id="local" type="runner" load="galaxy.jobs.runners.local:LocalJobRunner"/>
        <plugin id="slurm" type="runner" load="galaxy.jobs.runners.slurm:SlurmJobRunner"/>
     </plugins>

    <destinations default="local">
        <destination id ="local" runner="local"/>
        <destination id="slurm" runner="slurm" />
        <destination id="generate_slurm_job_params" runner="dynamic">
            <param id="type">python</param>
            <param id="function">integrate_job_destination_params</param>
        </destination>
        <destination id="transcriptome" runner="dynamic">
           <param id="type">python</param>
           <param id="function">is_user_in_role</param>
        </destination>
        <!-- This is a destination with fixed destination params - used to test connection to cluster -->
        <destination id="slurm_static" runner="slurm">
          <param id="nativeSpecification">--account=nn9108k --time=05:00 --mem-per-cpu=2 --nodes=1 --ntasks-per-node=1</param>
        </destination>
        <destination id="slurm_bigmem" runner="slurm">
          <param id="nativeSpecification">--account=nn9108k --time=24:05:00 --partition=bigmem --mem-per-cpu=12288 --nodes=1 --ntasks=4</param>
        </destination>
        <destination id="slurm_minimal" runner="slurm">
          <param id="nativeSpecification">--account=nn9108k --time=10:00 --mem-per-cpu=1024 --nodes=1 --ntasks=1</param>
        </destination>
     </destinations>
     
     <tools>
        <tool id="testing" destination="generate_slurm_job_params" resources="saga"/>
        <tool id="Module_test" destination="slurm_minimal" resources="saga"/>
        <tool id="ncbi_tblastn_wrapper" destination="slurm_bigmem"/>
    </tools>

    <limits>
        <limit type="registered_user_concurrent_jobs">5</limit>
        <limit type="anonymous_user_concurrent_jobs">1</limit>
        <limit type="destination_user_concurrent_jobs" id="local">5</limit>
        <limit type="destination_user_concurrent_jobs" tag="slurm">10</limit>
        <limit type="destination_total_concurrent_jobs" id="local">5</limit>
        <limit type="destination_total_concurrent_jobs" tag="longjobs">10</limit>
        <limit type="walltime">168:00:00</limit>
        <limit type="total_walltime" window="7">168:00:00</limit>
        <limit type="output_size">50GB</limit>
    </limits>
    
    <resources>
        <group id="saga">processors,memory,time,project</group>
   </resources>
    
 </job_conf>
