 <job_conf>

     <plugins workers="4">
        <plugin id="local" type="runner" load="galaxy.jobs.runners.local:LocalJobRunner"/>
        <plugin id="slurm" type="runner" load="galaxy.jobs.runners.slurm:SlurmJobRunner"/>
     </plugins>

    <destinations default="local">
        <destination id ="local" runner="local"/>
        <destination id="slurm" runner="slurm" />
        <!-- All tools shall collect the job destination parameters from the tool menu.
                  This is done by the following dynamic runner : generate_slurm_job_params.py
         -->
        <destination id="generate_slurm_job_params" runner="dynamic">
            <param id="type">python</param>
            <param id="function">integrate_job_destination_params</param>
        </destination>
        <destination id="transcriptome" runner="dynamic">
           <param id="type">python</param>
           <param id="function">is_user_in_role</param>
        </destination>
        <!-- This is a destination with fixed destination params - used to test connection to cluster -->
        <destination id="slurm_static" runner="slurm">
          <param id="nativeSpecification">--account=nn9108k --time=05:00 --mem-per-cpu=1 --nodes=1 --ntasks-per-node=1</param>
        </destination>
        <destination id="slurm_tool_test" runner="slurm">
          <param id="nativeSpecification">--account=nn9999k --time=1:05:00 --mem-per-cpu=4096 --partition=bigmem --nodes=1 --ntasks=16 </param>
        </destination>
     </destinations>
     
     <tools>
        <tool id="Module_test" destination="slurm_static" /> -->
        <tool id="ncbi_tblastn_wrapper" destination="slurm_tool_test" /> -->
        <tool id="testing" destination="generate_slurm_job_params" resources="dynamic_slurm_job_params"/>
        <tool id="testing_static_slurm" destination="slurm_static" />
        <tool id="tool_running_restricted" destination="transcriptome" required_role="transcriptome_users" final_destination="slurm" resources="testing_dynamic"/>
    </tools>
     â€“
    <limits>
        <!-- Certain limits can be defined. The 'concurrent_jobs' limits all
             control the number of jobs that can be "active" at a time, that
             is, dispatched to a runner and in the 'queued' or 'running'
             states.

             A race condition exists that will allow destination_* concurrency
             limits to be surpassed when multiple handlers are allowed to
             handle jobs for the same destination. To prevent this, assign all
             jobs for a specific destination to a single handler.
        -->
        <!-- registered_user_concurrent_jobs:
                Limit on the number of jobs a user with a registered Galaxy
                account can have active across all destinations.
        -->
        <limit type="registered_user_concurrent_jobs">5</limit>
        <!-- anonymous_user_concurrent_jobs:
                Likewise, but for unregistered/anonymous users.
        -->
        <limit type="anonymous_user_concurrent_jobs">1</limit>
        <!-- destination_user_concurrent_jobs:
                The number of jobs a user can have active in the specified
                destination, or across all destinations identified by the
                specified tag. (formerly: concurrent_jobs)
        -->
        <limit type="destination_user_concurrent_jobs" id="local">5</limit>
        <limit type="destination_user_concurrent_jobs" tag="slurm">10</limit>
        <!-- destination_total_concurrent_jobs:
                The number of jobs that can be active in the specified
                destination (or across all destinations identified by the
                specified tag) by any/all users.
        -->
        <limit type="destination_total_concurrent_jobs" id="local">5</limit>
        <limit type="destination_total_concurrent_jobs" tag="longjobs">10</limit>
        <!-- walltime:
                Amount of time a job can run (in any destination) before it
                will be terminated by Galaxy.
         -->
        <limit type="walltime">168:00:00</limit>
        <!-- total_walltime:
                Total walltime that jobs may not exceed during a set period.
                If total walltime of finished jobs exceeds this value, any
                new jobs are paused.  `window` is a number in days,
                representing the period.
         -->
        <limit type="total_walltime" window="7">168:00:00</limit>
        <!-- output_size:
                Size that any defined tool output can grow to before the job
                will be terminated. This does not include temporary files
                created by the job. Format is flexible, e.g.:
                '10GB' = '10g' = '10240 Mb' = '10737418240'
        -->
        <limit type="output_size">50GB</limit>
    </limits>
    
    <resources>
        <group id="testing_dynamic">processors,memory,time,project</group>
        <group id="dynamic_slurm_job_params">processors,memory,time,project</group>
   </resources>
    
 </job_conf>
